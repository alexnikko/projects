{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Work was done 23.12.2019`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## О чём\n",
    "### Ансамбли\n",
    "В этом ноутбуке реализованы алгоритмы бустинга и бэггинга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T18:09:58.492807Z",
     "start_time": "2019-12-23T18:09:56.431197Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.testing as npt\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.datasets import make_classification\n",
    "from scipy.optimize import minimize\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Сэмплирование случайных объектов и признаков\n",
    "\n",
    "Во многих ансамблевых алгоритмах используется прием, заключающийся в обучении на случайной подвыборке объектов или на случайном подмножестве признаков.\n",
    "\n",
    "Так что для начала реализуем класс, который будет упрощать семплирование различных подмассивов данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T18:09:58.509765Z",
     "start_time": "2019-12-23T18:09:58.494804Z"
    }
   },
   "outputs": [],
   "source": [
    "class BaseSampler:\n",
    "    def __init__(self, max_samples=1.0, bootstrap=False):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        bootstrap : Boolean\n",
    "            if True then use bootstrap sampling\n",
    "        max_samples : float in [0;1]\n",
    "            proportion of sampled examples\n",
    "        \"\"\"\n",
    "        self.bootstrap = bootstrap\n",
    "        self.max_samples = max_samples\n",
    "    \n",
    "    def sample(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class ObjectSampler(BaseSampler):\n",
    "    def __init__(self, axis=0, max_samples=1.0, bootstrap=True):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        axis : int\n",
    "            which axis use to sample\n",
    "        \"\"\"\n",
    "        self.axis = axis\n",
    "        super().__init__(max_samples=max_samples, bootstrap=bootstrap)\n",
    "    \n",
    "    def sample(self, x, y):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : numpy ndarray of shape (n_objects, n_features)\n",
    "        y : numpy ndarray of shape (n_objects,)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        x_sampled, y_sampled : numpy ndarrays of shape (n_samples, n_features) and (n_samples,)\n",
    "            n_samples = x_sampled.shape[0] * self.max_samples\n",
    "        \"\"\"\n",
    "        idxs = np.random.choice(x.shape[0], size=int(self.max_samples * x.shape[0]), replace=self.bootstrap)\n",
    "        x_sampled = np.take(x, idxs, axis=0)\n",
    "        y_sampled = np.take(y, idxs, axis=0)\n",
    "        return x_sampled, y_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T18:09:58.528717Z",
     "start_time": "2019-12-23T18:09:58.513754Z"
    }
   },
   "outputs": [],
   "source": [
    "class FeaturesSampler(BaseSampler):\n",
    "    def __init__(self, axis=1, max_samples=1.0, bootstrap=False):\n",
    "        self.axis = axis\n",
    "        super().__init__(max_samples=max_samples, bootstrap=bootstrap)\n",
    "        \n",
    "    def sample(self, x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : numpy ndarray of shape (n_objects, n_features)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        indices : numpy ndarrays of shape (n_features_sampled)\n",
    "        \"\"\"\n",
    "        indices = np.random.choice(x.shape[1], size=int(self.max_samples * x.shape[1]), replace=self.bootstrap)\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим себя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T18:09:58.555642Z",
     "start_time": "2019-12-23T18:09:58.534700Z"
    }
   },
   "outputs": [],
   "source": [
    "some_X = np.array([[0, 1, 2], [0.3, 1, 3], [0.5, 1, 3], [1, 2, 1]])\n",
    "some_y = np.array([1, 5, 3, 1])\n",
    "\n",
    "object_sampler = ObjectSampler(max_samples=0.7)\n",
    "feature_sampler = FeaturesSampler(max_samples=0.7)\n",
    "\n",
    "assert object_sampler.sample(some_X, some_y)[0].shape == (int(0.7*some_X.shape[0]), some_X.shape[1])\n",
    "assert object_sampler.sample(some_X, some_y)[1].shape == (int(0.7*some_y.shape[0]),)\n",
    "\n",
    "sample_X, sample_y = object_sampler.sample(some_X, some_y)\n",
    "\n",
    "for sub_x, sub_y in zip(sample_X, sample_y):\n",
    "    assert sub_x.tolist() in some_X.tolist()\n",
    "    assert sub_y in sample_y\n",
    "\n",
    "assert feature_sampler.sample(some_X).shape == (int(0.7*some_X.shape[1]),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Бэггинг\n",
    "\n",
    "Суть бэггинга заключается в обучении нескольких 'слабых' базовых моделей и объединении их в одну модель, обладающую бОльшей обобщающей способностью. Каждая базовая модель обучается на случайно выбранном подмножестве объектов и на случайно выбранном подмножестве признаков для этих объектов.\n",
    "\n",
    "Реализуем класс `Bagger`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T18:09:58.578581Z",
     "start_time": "2019-12-23T18:09:58.558634Z"
    }
   },
   "outputs": [],
   "source": [
    "class Bagger:\n",
    "    def __init__(\n",
    "        self, base_estimator,\n",
    "        object_sampler, feature_sampler,\n",
    "        n_estimators=10, **params\n",
    "    ):\n",
    "        \"\"\"\n",
    "        n_estimators : int\n",
    "            number of base estimators\n",
    "        base_estimator : class\n",
    "            class for base_estimator with fit(), predict() and predict_proba() methods\n",
    "        feature_sampler : instance of FeaturesSampler\n",
    "        object_sampler : instance of ObjectSampler\n",
    "        estimators : list\n",
    "            list for containing base_estimator instances\n",
    "        indices : list\n",
    "            list for containing feature indices for each estimator\n",
    "        params : kwargs\n",
    "            params for base_estimator initialization\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.base_estimator = base_estimator\n",
    "        self.feature_sampler = feature_sampler\n",
    "        self.object_sampler = object_sampler\n",
    "        self.estimators = []\n",
    "        self.indices = []\n",
    "        self.params = params\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        for i in range(self.n_estimators):\n",
    "            1) select random indices of features for current estimator\n",
    "            2) select random objects and answers for train\n",
    "            3) fit base_estimator (don't forget to remain only selected features)\n",
    "            4) save base_estimator (self.estimators) and feature indices (self.indices)\n",
    "        \n",
    "        NOTE that self.base_estimator is class and you should init it with\n",
    "        self.base_estimator(**self.params) before fitting\n",
    "        \"\"\"\n",
    "        for i in range(self.n_estimators):\n",
    "            indices = self.feature_sampler.sample(X)\n",
    "            X_sample, y_sample = self.object_sampler.sample(X, y)\n",
    "            estimator = self.base_estimator(**self.params)\n",
    "            estimator.fit(np.take(X, indices, axis=1), y)\n",
    "            self.estimators.append(estimator)\n",
    "            self.indices.append(indices)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        probas : numpy ndarrays of shape (n_objects, n_classes)\n",
    "        \n",
    "        Calculate mean value of all probas from base_estimators\n",
    "        Don't forget, that each estimator has its own feature indices for prediction\n",
    "        \"\"\"\n",
    "        probas = np.array([estim.predict_proba(np.take(X, idxs, axis=1)) for estim, idxs in \n",
    "                          zip(self.estimators, self.indices)])\n",
    "        probas = np.mean(probas, axis=0)\n",
    "        return probas\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        predictions : numpy ndarrays of shape (n_objects, )\n",
    "        \n",
    "        \"\"\"\n",
    "        probas = self.predict_proba(X)\n",
    "        predictions = np.argmax(probas, axis=1)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для проверки, обучим бэггинг над решающими деревьями (случайный лес)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T18:09:59.133792Z",
     "start_time": "2019-12-23T18:09:59.124817Z"
    }
   },
   "outputs": [],
   "source": [
    "class RandomForestClassifier(Bagger):\n",
    "    def __init__(self, n_estimators=30, max_depth=None, min_samples_leaf=1):\n",
    "        base_estimator = DecisionTreeClassifier\n",
    "        objects_sampler = ObjectSampler(max_samples=0.9)\n",
    "        features_sampler = FeaturesSampler(max_samples=0.8)\n",
    "        \n",
    "        super().__init__(\n",
    "            base_estimator=base_estimator,\n",
    "            object_sampler=object_sampler,\n",
    "            feature_sampler=feature_sampler,\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_leaf=min_samples_leaf\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T18:09:59.766379Z",
     "start_time": "2019-12-23T18:09:59.708534Z"
    }
   },
   "outputs": [],
   "source": [
    "some_random_forest = RandomForestClassifier()\n",
    "\n",
    "some_X, some_y = make_classification(n_samples=30, n_features=50,\n",
    "                                     n_informative=50, n_redundant=0,\n",
    "                                     random_state=0, shuffle=False)\n",
    "\n",
    "some_random_forest.fit(some_X, some_y)\n",
    "predictions = some_random_forest.predict(some_X)\n",
    "assert isinstance(predictions, type(np.zeros(0)))\n",
    "npt.assert_equal(predictions, some_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Градиентный бустинг\n",
    "\n",
    "Бустинг последовательно обучает набор базовых моделей таким образом, что каждая следующая модель пытается исправить ошибки работы предыдущей модели. Логика того, как учитываются ошибки предыдущей модели может быть разной. В алгоритме градиентного бустинга каждая следующая модель обучается на \"невязках\" предыдущей модели, минимизируя итоговую функцию потерь. У каждого следующего алгоритма вычисляется вес $\\alpha$, с которым он входит в ансамбль. Также есть параметр скорости обучения (learning rate), который не позволяет алгоритму переобучиться. Вес $\\alpha$ можно находить, используя одномерную оптимизацию. Можно записать процедуру обучения по шагам (будем рассматривать случай бинарной классификации c метками классов {0,1}, чтобы не усложнять жизнь):\n",
    "1. Настройка базового алгоритма $b_0$.\n",
    "    \n",
    "    Алгоритм настраиваются на $y$ с помощью функции MSE.\n",
    "    \n",
    "    \n",
    "2. Будем обозначать текущий небазовый алгоритм - $a$:\n",
    "    \n",
    "    $$a_i(x) = \\sum_{j=0}^i \\alpha_j b_j(x) $$\n",
    "    \n",
    "3. Настройка базового алгоритма $b_i$ (обычно это регрессионное дерево):\n",
    "    \n",
    "    $$b_i = \\arg \\min_b \\sum_{j=1}^l (b(x_j) + \\nabla L(a_{i-1}(x_j), y))^2,$$\n",
    "    т.е. выход очередного базового алгоритма подстраивается под антиградиент функции потерь\n",
    "    \n",
    "4. Настройка веса базового алгоритма $\\alpha_i$:\n",
    "    \n",
    "    $$\\alpha_i = \\min_{\\alpha > 0} \\sum_{j=1}^l L(a_{i-1} + \\alpha b_i(x_j), y) $$\n",
    "    \n",
    "В случае классфикации будем использовать логистическую функцию потерь. Немного упростим ее:\n",
    "\n",
    "$$L = -y\\log\\sigma(a) - (1-y)\\log(1 - \\sigma(a)) = -\\log(1 - \\sigma(a)) - y \\log \\frac{\\sigma(a)}{1 - \\sigma(a)},$$\n",
    "где $\\sigma$ - функция сигмоиды. Ответ после очередного базового алгоритма надо прогонять через сигмоиду, т.к. не гарантируется, что ответы будут лежать на [0,1] - в этом особенность базового алгоритма (который является регрессионным).\n",
    "\n",
    "Преобразуем:\n",
    "$$\\log (1 - \\sigma(a)) = \\log \\frac{1}{1 + \\exp(a)} = -\\log(1 + \\exp(a)) $$\n",
    "\n",
    "$$\\log (\\frac{\\sigma(a)}{1 - \\sigma(a)}) = \\log(\\exp(a)) = a $$\n",
    " \n",
    "Таким образом:\n",
    "\n",
    "$$L = -ya + \\log(1 + \\exp(a))$$\n",
    "\n",
    "Тогда будем вычислять градиент как:\n",
    " \n",
    "$$\\nabla L = - y + \\sigma(a)$$\n",
    "\n",
    "Реализуем стратегию обучения базовых классификаторов для класса `GradientBoostingClassifier`\n",
    "\n",
    "А также класс `BoosterClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T18:10:02.592924Z",
     "start_time": "2019-12-23T18:10:02.572975Z"
    }
   },
   "outputs": [],
   "source": [
    "class BoosterClassifier:\n",
    "    def __init__(\n",
    "        self, base_estimator, feature_sampler,\n",
    "        n_estimators=10, lr=.5, **params\n",
    "    ):\n",
    "        \"\"\"\n",
    "        n_estimators : int\n",
    "            number of base estimators\n",
    "        base_estimator : class\n",
    "            class for base_estimator with fit(), predict() and predict_proba() methods\n",
    "        feature_sampler : instance of FeaturesSampler\n",
    "        estimators : list\n",
    "            list for containing base_estimator instances\n",
    "        indices : list\n",
    "            list for containing feature indices for each estimator\n",
    "        weights : list\n",
    "            list for containing estimators weights\n",
    "        lr : float\n",
    "            learning rate for estimators\n",
    "        params : kwargs\n",
    "            kwargs for base_estimator init\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.base_estimator = base_estimator\n",
    "        self.feature_sampler = feature_sampler\n",
    "        self.estimators = []\n",
    "        self.indices = []\n",
    "        self.lr = lr\n",
    "        self.params = params\n",
    "        self.weights = []\n",
    "    \n",
    "    def _fit_first_estimator(self, X, y):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def _fit_base_estimator(self, X, y):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X : X : numpy ndarray of shape (n_objects, n_features)\n",
    "        y : numpy ndarray of shape (n_objects, )\n",
    "        -------\n",
    "        iteratively fits base models\n",
    "        \"\"\"\n",
    "        self._fit_first_estimator(X, y)\n",
    "        predictions_base = self.estimators[-1].predict(np.take(X, self.indices[-1], axis=1)) * self.weights[0]\n",
    "        for i in range(self.n_estimators - 1):\n",
    "            predictions_base = self._fit_base_estimator(X, y, predictions_base)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        probas : numpy ndarray of shape (n_objects, )\n",
    "            predictions for one class\n",
    "        -------\n",
    "        1) get predictions by first model (self.estimators[0])\n",
    "        2) for each estimator in self.estimators[1:] (and its indicies and weights):\n",
    "            update predictions\n",
    "        3) turn into probability distribution with sigmoid function\n",
    "        \"\"\"\n",
    "        probas = 0\n",
    "        for estimator, weight, idxs in zip(self.estimators, self.weights, self.indices):\n",
    "            probas += weight * estimator.predict(np.take(X, idxs, axis=1))\n",
    "        probas = expit(probas)\n",
    "        return probas\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probas = self.predict_proba(X)\n",
    "        return (probas > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T18:10:03.207096Z",
     "start_time": "2019-12-23T18:10:03.188149Z"
    }
   },
   "outputs": [],
   "source": [
    "class GradientBoostingClassifier(BoosterClassifier):\n",
    "    \n",
    "    def __init__(self, n_estimators=30, lr=0.5, max_depth=2, min_samples_leaf=1):\n",
    "        \"\"\"\n",
    "        n_estimators : int\n",
    "            number of base estimators\n",
    "        base_estimator : class\n",
    "            class for base_estimator with fit(), predict() and predict_proba() methods\n",
    "        feature_sampler : instance of FeaturesSampler\n",
    "        estimators : list\n",
    "            list for containing base_estimator instances\n",
    "        indices : list\n",
    "            list for containing feature indices for each estimator\n",
    "        lr : float\n",
    "            learning rate for estimators\n",
    "        params : kwargs\n",
    "            kwargs for base_estimator init\n",
    "        \"\"\"\n",
    "        \n",
    "        base_estimator = DecisionTreeRegressor\n",
    "        feature_sampler = FeaturesSampler(max_samples=0.8)\n",
    "        super().__init__(\n",
    "            base_estimator=base_estimator,\n",
    "            feature_sampler=feature_sampler,\n",
    "            n_estimators=n_estimators,\n",
    "            lr=lr,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_leaf=min_samples_leaf\n",
    "        )\n",
    "        \n",
    "    def log_loss_gradient(self, y, pred):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        gradient : numpy ndarrays of shape (n_objects, )\n",
    "        \"\"\"\n",
    "        gradient = expit(pred) - y\n",
    "        return gradient\n",
    "    \n",
    "    def log_loss(self, y, pred):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        loss : log loss for inputs\n",
    "        \"\"\"\n",
    "        loss = -y * pred + np.logaddexp(0, pred)\n",
    "        return loss\n",
    "    \n",
    "    def _fit_first_estimator(self, X, y):\n",
    "        \"\"\"\n",
    "        1) select random indices of features for first estimator\n",
    "        2) fit base_estimator (don't forget to remain only selected features)\n",
    "        3) calculate optimal weight for estimator by optimization\n",
    "        4) save base_estimator (self.estimators), feature indices (self.indices) and weight (self.weights)\n",
    "        \n",
    "        NOTE that self.base_estimator is class and you should init it with\n",
    "        self.base_estimator(**self.params) before fitting\n",
    "        \"\"\"\n",
    "        idxs = self.feature_sampler.sample(X)\n",
    "        estimator = self.base_estimator(**self.params).fit(np.take(X, idxs, axis=1), y)\n",
    "        weight = 1\n",
    "        \n",
    "        self.weights.append(weight)\n",
    "        self.indices.append(idxs)\n",
    "        self.estimators.append(estimator)\n",
    "    \n",
    "    def _fit_base_estimator(self, X, y, predictions_base):\n",
    "        \"\"\"\n",
    "        X : numpy ndarrays of shape (n_objects, n_features)\n",
    "        y : numpy ndarrays of shape (n_objects, )\n",
    "        predictions_base : numpy ndarrays of shape (n_objects, n_classes)\n",
    "            updated predictions from previous step\n",
    "        -------\n",
    "        Returns\n",
    "        -------\n",
    "        y_updated : numpy ndarrays of shape (n_objects, n_classes)\n",
    "            updated predictions\n",
    "        -------\n",
    "        \n",
    "        1) calculate gradient\n",
    "        2) select random indices of features for current estimator\n",
    "        3) fit estimator with predictions_base as target\n",
    "        4) calculate optimal weight for estimator by optimization\n",
    "        5) calculate y_updated\n",
    "        6) save estimator, indicies and weights\n",
    "        \n",
    "        NOTE that self.base_estimator is class and you should init it with\n",
    "        self.base_estimator(**self.params) before fitting\n",
    "        \"\"\"\n",
    "        gradient = self.log_loss_gradient(y, predictions_base)\n",
    "        idxs = self.feature_sampler.sample(X)\n",
    "        estimator = self.base_estimator(**self.params).fit(np.take(X, idxs, axis=1), gradient)\n",
    "        pred = estimator.predict(np.take(X, idxs, axis=1))\n",
    "        func_to_optimize = lambda x: np.sum(self.log_loss(y, predictions_base + x * pred))\n",
    "        weight = minimize(func_to_optimize, 1).x[0] * self.lr\n",
    "        y_updated = predictions_base + weight * pred\n",
    "        self.estimators.append(estimator)\n",
    "        self.weights.append(weight)\n",
    "        self.indices.append(idxs)\n",
    "        return y_updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим себя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T18:10:03.839966Z",
     "start_time": "2019-12-23T18:10:03.805061Z"
    }
   },
   "outputs": [],
   "source": [
    "some_gradient_classifier = GradientBoostingClassifier()\n",
    "some_gradient_classifier.fit(some_X, some_y)\n",
    "predictions = some_gradient_classifier.predict(some_X)\n",
    "assert isinstance(predictions, type(np.zeros(0)))\n",
    "npt.assert_equal(predictions, some_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эксперименты\n",
    "\n",
    "Скачаем датасейт для экспериментов [отсюда](https://www.kaggle.com/jsphyg/weather-dataset-rattle-package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:45:08.954788Z",
     "start_time": "2019-12-23T19:45:08.951762Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:45:09.759093Z",
     "start_time": "2019-12-23T19:45:09.228549Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('weatherAUS.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделим признаки год/месяц/день:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:45:09.873929Z",
     "start_time": "2019-12-23T19:45:09.838064Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustDir</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindDir9am</th>\n",
       "      <th>...</th>\n",
       "      <th>Humidity3pm</th>\n",
       "      <th>Pressure9am</th>\n",
       "      <th>Pressure3pm</th>\n",
       "      <th>Cloud9am</th>\n",
       "      <th>Cloud3pm</th>\n",
       "      <th>Temp9am</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>RISK_MM</th>\n",
       "      <th>RainTomorrow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-12-01</td>\n",
       "      <td>Albury</td>\n",
       "      <td>13.4</td>\n",
       "      <td>22.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>44.0</td>\n",
       "      <td>W</td>\n",
       "      <td>...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1007.7</td>\n",
       "      <td>1007.1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.9</td>\n",
       "      <td>21.8</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-12-02</td>\n",
       "      <td>Albury</td>\n",
       "      <td>7.4</td>\n",
       "      <td>25.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WNW</td>\n",
       "      <td>44.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>...</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1010.6</td>\n",
       "      <td>1007.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.2</td>\n",
       "      <td>24.3</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-12-03</td>\n",
       "      <td>Albury</td>\n",
       "      <td>12.9</td>\n",
       "      <td>25.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WSW</td>\n",
       "      <td>46.0</td>\n",
       "      <td>W</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1007.6</td>\n",
       "      <td>1008.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-12-04</td>\n",
       "      <td>Albury</td>\n",
       "      <td>9.2</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NE</td>\n",
       "      <td>24.0</td>\n",
       "      <td>SE</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1017.6</td>\n",
       "      <td>1012.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.1</td>\n",
       "      <td>26.5</td>\n",
       "      <td>No</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-12-05</td>\n",
       "      <td>Albury</td>\n",
       "      <td>17.5</td>\n",
       "      <td>32.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>41.0</td>\n",
       "      <td>ENE</td>\n",
       "      <td>...</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1010.8</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>29.7</td>\n",
       "      <td>No</td>\n",
       "      <td>0.2</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date Location  MinTemp  MaxTemp  Rainfall  Evaporation  Sunshine  \\\n",
       "0  2008-12-01   Albury     13.4     22.9       0.6          NaN       NaN   \n",
       "1  2008-12-02   Albury      7.4     25.1       0.0          NaN       NaN   \n",
       "2  2008-12-03   Albury     12.9     25.7       0.0          NaN       NaN   \n",
       "3  2008-12-04   Albury      9.2     28.0       0.0          NaN       NaN   \n",
       "4  2008-12-05   Albury     17.5     32.3       1.0          NaN       NaN   \n",
       "\n",
       "  WindGustDir  WindGustSpeed WindDir9am  ... Humidity3pm  Pressure9am  \\\n",
       "0           W           44.0          W  ...        22.0       1007.7   \n",
       "1         WNW           44.0        NNW  ...        25.0       1010.6   \n",
       "2         WSW           46.0          W  ...        30.0       1007.6   \n",
       "3          NE           24.0         SE  ...        16.0       1017.6   \n",
       "4           W           41.0        ENE  ...        33.0       1010.8   \n",
       "\n",
       "   Pressure3pm  Cloud9am  Cloud3pm  Temp9am  Temp3pm  RainToday  RISK_MM  \\\n",
       "0       1007.1       8.0       NaN     16.9     21.8         No      0.0   \n",
       "1       1007.8       NaN       NaN     17.2     24.3         No      0.0   \n",
       "2       1008.7       NaN       2.0     21.0     23.2         No      0.0   \n",
       "3       1012.8       NaN       NaN     18.1     26.5         No      1.0   \n",
       "4       1006.0       7.0       8.0     17.8     29.7         No      0.2   \n",
       "\n",
       "   RainTomorrow  \n",
       "0            No  \n",
       "1            No  \n",
       "2            No  \n",
       "3            No  \n",
       "4            No  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:45:10.301882Z",
     "start_time": "2019-12-23T19:45:10.039584Z"
    }
   },
   "outputs": [],
   "source": [
    "data['year'] = data['Date'].apply(lambda x: x.split('-')[0])\n",
    "data['month'] = data['Date'].apply(lambda x: x.split('-')[1])\n",
    "data['day'] = data['Date'].apply(lambda x: x.split('-')[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:45:10.362718Z",
     "start_time": "2019-12-23T19:45:10.304873Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year month day\n",
       "0  2008    12  01\n",
       "1  2008    12  02\n",
       "2  2008    12  03\n",
       "3  2008    12  04\n",
       "4  2008    12  05"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['year', 'month', 'day']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим какие года есть в выборке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:45:10.495421Z",
     "start_time": "2019-12-23T19:45:10.463451Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2016    17508\n",
       "2014    17400\n",
       "2015    17231\n",
       "2009    16595\n",
       "2010    16419\n",
       "2013    16097\n",
       "2011    15126\n",
       "2012    15044\n",
       "2017     8466\n",
       "2008     2246\n",
       "2007       61\n",
       "Name: year, dtype: int64"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим выборку на три части (train, val и test) по временному принципу:\n",
    "    \n",
    "* train - 2007-2014\n",
    "* val - 2015\n",
    "* test - 2016-2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:45:10.893298Z",
     "start_time": "2019-12-23T19:45:10.748689Z"
    }
   },
   "outputs": [],
   "source": [
    "data['year'] = data['year'].astype(int)\n",
    "data['month'] = data['month'].astype(int)\n",
    "data['day'] = data['day'].astype(int)\n",
    "\n",
    "indexes = {\n",
    "    'train': (data['year'] >= 2007) & (data['year'] <= 2014),\n",
    "    'val': data['year'] == 2015,\n",
    "    'test': (data['year'] == 2016) | (data['year'] == 2017)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:45:11.125528Z",
     "start_time": "2019-12-23T19:45:11.088590Z"
    }
   },
   "outputs": [],
   "source": [
    "data.drop(['Date'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:45:11.297206Z",
     "start_time": "2019-12-23T19:45:11.266288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Evaporation</th>\n",
       "      <th>Sunshine</th>\n",
       "      <th>WindGustDir</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindDir9am</th>\n",
       "      <th>WindDir3pm</th>\n",
       "      <th>...</th>\n",
       "      <th>Cloud9am</th>\n",
       "      <th>Cloud3pm</th>\n",
       "      <th>Temp9am</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>RISK_MM</th>\n",
       "      <th>RainTomorrow</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Albury</td>\n",
       "      <td>13.4</td>\n",
       "      <td>22.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>44.0</td>\n",
       "      <td>W</td>\n",
       "      <td>WNW</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.9</td>\n",
       "      <td>21.8</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albury</td>\n",
       "      <td>7.4</td>\n",
       "      <td>25.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WNW</td>\n",
       "      <td>44.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>WSW</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.2</td>\n",
       "      <td>24.3</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Albury</td>\n",
       "      <td>12.9</td>\n",
       "      <td>25.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WSW</td>\n",
       "      <td>46.0</td>\n",
       "      <td>W</td>\n",
       "      <td>WSW</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>No</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No</td>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Albury</td>\n",
       "      <td>9.2</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NE</td>\n",
       "      <td>24.0</td>\n",
       "      <td>SE</td>\n",
       "      <td>E</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.1</td>\n",
       "      <td>26.5</td>\n",
       "      <td>No</td>\n",
       "      <td>1.0</td>\n",
       "      <td>No</td>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Albury</td>\n",
       "      <td>17.5</td>\n",
       "      <td>32.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W</td>\n",
       "      <td>41.0</td>\n",
       "      <td>ENE</td>\n",
       "      <td>NW</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>29.7</td>\n",
       "      <td>No</td>\n",
       "      <td>0.2</td>\n",
       "      <td>No</td>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Location  MinTemp  MaxTemp  Rainfall  Evaporation  Sunshine WindGustDir  \\\n",
       "0   Albury     13.4     22.9       0.6          NaN       NaN           W   \n",
       "1   Albury      7.4     25.1       0.0          NaN       NaN         WNW   \n",
       "2   Albury     12.9     25.7       0.0          NaN       NaN         WSW   \n",
       "3   Albury      9.2     28.0       0.0          NaN       NaN          NE   \n",
       "4   Albury     17.5     32.3       1.0          NaN       NaN           W   \n",
       "\n",
       "   WindGustSpeed WindDir9am WindDir3pm  ...  Cloud9am  Cloud3pm  Temp9am  \\\n",
       "0           44.0          W        WNW  ...       8.0       NaN     16.9   \n",
       "1           44.0        NNW        WSW  ...       NaN       NaN     17.2   \n",
       "2           46.0          W        WSW  ...       NaN       2.0     21.0   \n",
       "3           24.0         SE          E  ...       NaN       NaN     18.1   \n",
       "4           41.0        ENE         NW  ...       7.0       8.0     17.8   \n",
       "\n",
       "   Temp3pm  RainToday  RISK_MM  RainTomorrow  year  month  day  \n",
       "0     21.8         No      0.0            No  2008     12    1  \n",
       "1     24.3         No      0.0            No  2008     12    2  \n",
       "2     23.2         No      0.0            No  2008     12    3  \n",
       "3     26.5         No      1.0            No  2008     12    4  \n",
       "4     29.7         No      0.2            No  2008     12    5  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:45:11.582445Z",
     "start_time": "2019-12-23T19:45:11.451795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 142193 entries, 0 to 142192\n",
      "Data columns (total 26 columns):\n",
      "Location         142193 non-null object\n",
      "MinTemp          141556 non-null float64\n",
      "MaxTemp          141871 non-null float64\n",
      "Rainfall         140787 non-null float64\n",
      "Evaporation      81350 non-null float64\n",
      "Sunshine         74377 non-null float64\n",
      "WindGustDir      132863 non-null object\n",
      "WindGustSpeed    132923 non-null float64\n",
      "WindDir9am       132180 non-null object\n",
      "WindDir3pm       138415 non-null object\n",
      "WindSpeed9am     140845 non-null float64\n",
      "WindSpeed3pm     139563 non-null float64\n",
      "Humidity9am      140419 non-null float64\n",
      "Humidity3pm      138583 non-null float64\n",
      "Pressure9am      128179 non-null float64\n",
      "Pressure3pm      128212 non-null float64\n",
      "Cloud9am         88536 non-null float64\n",
      "Cloud3pm         85099 non-null float64\n",
      "Temp9am          141289 non-null float64\n",
      "Temp3pm          139467 non-null float64\n",
      "RainToday        140787 non-null object\n",
      "RISK_MM          142193 non-null float64\n",
      "RainTomorrow     142193 non-null object\n",
      "year             142193 non-null int32\n",
      "month            142193 non-null int32\n",
      "day              142193 non-null int32\n",
      "dtypes: float64(17), int32(3), object(6)\n",
      "memory usage: 26.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### дропнем колонки с большим количеством пропусков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:45:11.746782Z",
     "start_time": "2019-12-23T19:45:11.714869Z"
    }
   },
   "outputs": [],
   "source": [
    "data.drop(['Evaporation', 'Sunshine', 'Cloud9am', 'Cloud3pm'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:45:12.173629Z",
     "start_time": "2019-12-23T19:45:12.088854Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Location         False\n",
       "MinTemp           True\n",
       "MaxTemp           True\n",
       "Rainfall          True\n",
       "WindGustDir       True\n",
       "WindGustSpeed     True\n",
       "WindDir9am        True\n",
       "WindDir3pm        True\n",
       "WindSpeed9am      True\n",
       "WindSpeed3pm      True\n",
       "Humidity9am       True\n",
       "Humidity3pm       True\n",
       "Pressure9am       True\n",
       "Pressure3pm       True\n",
       "Temp9am           True\n",
       "Temp3pm           True\n",
       "RainToday         True\n",
       "RISK_MM          False\n",
       "RainTomorrow     False\n",
       "year             False\n",
       "month            False\n",
       "day              False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:45:12.535356Z",
     "start_time": "2019-12-23T19:45:12.427673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 142193 entries, 0 to 142192\n",
      "Data columns (total 22 columns):\n",
      "Location         142193 non-null object\n",
      "MinTemp          141556 non-null float64\n",
      "MaxTemp          141871 non-null float64\n",
      "Rainfall         140787 non-null float64\n",
      "WindGustDir      132863 non-null object\n",
      "WindGustSpeed    132923 non-null float64\n",
      "WindDir9am       132180 non-null object\n",
      "WindDir3pm       138415 non-null object\n",
      "WindSpeed9am     140845 non-null float64\n",
      "WindSpeed3pm     139563 non-null float64\n",
      "Humidity9am      140419 non-null float64\n",
      "Humidity3pm      138583 non-null float64\n",
      "Pressure9am      128179 non-null float64\n",
      "Pressure3pm      128212 non-null float64\n",
      "Temp9am          141289 non-null float64\n",
      "Temp3pm          139467 non-null float64\n",
      "RainToday        140787 non-null object\n",
      "RISK_MM          142193 non-null float64\n",
      "RainTomorrow     142193 non-null object\n",
      "year             142193 non-null int32\n",
      "month            142193 non-null int32\n",
      "day              142193 non-null int32\n",
      "dtypes: float64(13), int32(3), object(6)\n",
      "memory usage: 22.2+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### разобьем колонки на категории и чиселки\n",
    "заполним пропуски в категориях значением 'unknown'\n",
    "\n",
    "а чиселки заполним средним"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:45:14.284990Z",
     "start_time": "2019-12-23T19:45:14.279009Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_columns = ['Location', 'WindGustDir', 'WindDir9am', 'WindDir3pm', 'RainToday', 'RainTomorrow']\n",
    "num_columns = list(set(data.columns) - set(cat_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:45:14.549180Z",
     "start_time": "2019-12-23T19:45:14.447451Z"
    }
   },
   "outputs": [],
   "source": [
    "data[num_columns] = data[num_columns].fillna(data[num_columns].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:45:14.739669Z",
     "start_time": "2019-12-23T19:45:14.613010Z"
    }
   },
   "outputs": [],
   "source": [
    "data[cat_columns] = data[cat_columns].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:45:14.839440Z",
     "start_time": "2019-12-23T19:45:14.758656Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Location         False\n",
       "MinTemp          False\n",
       "MaxTemp          False\n",
       "Rainfall         False\n",
       "WindGustDir      False\n",
       "WindGustSpeed    False\n",
       "WindDir9am       False\n",
       "WindDir3pm       False\n",
       "WindSpeed9am     False\n",
       "WindSpeed3pm     False\n",
       "Humidity9am      False\n",
       "Humidity3pm      False\n",
       "Pressure9am      False\n",
       "Pressure3pm      False\n",
       "Temp9am          False\n",
       "Temp3pm          False\n",
       "RainToday        False\n",
       "RISK_MM          False\n",
       "RainTomorrow     False\n",
       "year             False\n",
       "month            False\n",
       "day              False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:45:16.023744Z",
     "start_time": "2019-12-23T19:45:16.008748Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Albury', 'BadgerysCreek', 'Cobar', 'CoffsHarbour', 'Moree',\n",
       "       'Newcastle', 'NorahHead', 'NorfolkIsland', 'Penrith', 'Richmond',\n",
       "       'Sydney', 'SydneyAirport', 'WaggaWagga', 'Williamtown',\n",
       "       'Wollongong', 'Canberra', 'Tuggeranong', 'MountGinini', 'Ballarat',\n",
       "       'Bendigo', 'Sale', 'MelbourneAirport', 'Melbourne', 'Mildura',\n",
       "       'Nhil', 'Portland', 'Watsonia', 'Dartmoor', 'Brisbane', 'Cairns',\n",
       "       'GoldCoast', 'Townsville', 'Adelaide', 'MountGambier', 'Nuriootpa',\n",
       "       'Woomera', 'Albany', 'Witchcliffe', 'PearceRAAF', 'PerthAirport',\n",
       "       'Perth', 'SalmonGums', 'Walpole', 'Hobart', 'Launceston',\n",
       "       'AliceSprings', 'Darwin', 'Katherine', 'Uluru'], dtype=object)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Location'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### приведем object to cat, чтобы можно было использовать лейбл энкодинг из коробки пандаса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:45:17.694039Z",
     "start_time": "2019-12-23T19:45:17.583319Z"
    }
   },
   "outputs": [],
   "source": [
    "data[cat_columns] = data[cat_columns].astype('category', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:45:17.914229Z",
     "start_time": "2019-12-23T19:45:17.904255Z"
    }
   },
   "outputs": [],
   "source": [
    "for c in cat_columns:\n",
    "    data[c] = data[c].cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### убеждаемся, что всё закодировалось как надо"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:45:19.189299Z",
     "start_time": "2019-12-23T19:45:19.153395Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>WindGustDir</th>\n",
       "      <th>WindGustSpeed</th>\n",
       "      <th>WindDir9am</th>\n",
       "      <th>WindDir3pm</th>\n",
       "      <th>WindSpeed9am</th>\n",
       "      <th>WindSpeed3pm</th>\n",
       "      <th>...</th>\n",
       "      <th>Pressure9am</th>\n",
       "      <th>Pressure3pm</th>\n",
       "      <th>Temp9am</th>\n",
       "      <th>Temp3pm</th>\n",
       "      <th>RainToday</th>\n",
       "      <th>RISK_MM</th>\n",
       "      <th>RainTomorrow</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>13.4</td>\n",
       "      <td>22.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>14</td>\n",
       "      <td>44.0</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>20.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1007.7</td>\n",
       "      <td>1007.1</td>\n",
       "      <td>16.9</td>\n",
       "      <td>21.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7.4</td>\n",
       "      <td>25.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15</td>\n",
       "      <td>44.0</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>4.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1010.6</td>\n",
       "      <td>1007.8</td>\n",
       "      <td>17.2</td>\n",
       "      <td>24.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>12.9</td>\n",
       "      <td>25.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>46.0</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>19.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1007.6</td>\n",
       "      <td>1008.7</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>9.2</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>24.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1017.6</td>\n",
       "      <td>1012.8</td>\n",
       "      <td>18.1</td>\n",
       "      <td>26.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>17.5</td>\n",
       "      <td>32.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1010.8</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>29.7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Location  MinTemp  MaxTemp  Rainfall  WindGustDir  WindGustSpeed  \\\n",
       "0         2     13.4     22.9       0.6           14           44.0   \n",
       "1         2      7.4     25.1       0.0           15           44.0   \n",
       "2         2     12.9     25.7       0.0           16           46.0   \n",
       "3         2      9.2     28.0       0.0            4           24.0   \n",
       "4         2     17.5     32.3       1.0           14           41.0   \n",
       "\n",
       "   WindDir9am  WindDir3pm  WindSpeed9am  WindSpeed3pm  ...  Pressure9am  \\\n",
       "0          14          15          20.0          24.0  ...       1007.7   \n",
       "1           6          16           4.0          22.0  ...       1010.6   \n",
       "2          14          16          19.0          26.0  ...       1007.6   \n",
       "3           9           0          11.0           9.0  ...       1017.6   \n",
       "4           1           7           7.0          20.0  ...       1010.8   \n",
       "\n",
       "   Pressure3pm  Temp9am  Temp3pm  RainToday  RISK_MM  RainTomorrow  year  \\\n",
       "0       1007.1     16.9     21.8          0      0.0             0  2008   \n",
       "1       1007.8     17.2     24.3          0      0.0             0  2008   \n",
       "2       1008.7     21.0     23.2          0      0.0             0  2008   \n",
       "3       1012.8     18.1     26.5          0      1.0             0  2008   \n",
       "4       1006.0     17.8     29.7          0      0.2             0  2008   \n",
       "\n",
       "   month  day  \n",
       "0     12    1  \n",
       "1     12    2  \n",
       "2     12    3  \n",
       "3     12    4  \n",
       "4     12    5  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таргет - `RainTommorow`. Удалим его из обучающих данных, также удалим признак `RISK_MM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:45:21.059785Z",
     "start_time": "2019-12-23T19:45:21.039838Z"
    }
   },
   "outputs": [],
   "source": [
    "target_data = data['RainTomorrow']\n",
    "data.drop(['RainTomorrow', 'RISK_MM'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "разбиваем на траин, вал и тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:45:24.780283Z",
     "start_time": "2019-12-23T19:45:24.752316Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train = data[indexes['train']], target_data[indexes['train']]\n",
    "X_val, y_val = data[indexes['val']], target_data[indexes['val']]\n",
    "X_test, y_test = data[indexes['test']], target_data[indexes['test']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "первая проба"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T20:01:04.942574Z",
     "start_time": "2019-12-19T20:00:54.130812Z"
    }
   },
   "outputs": [],
   "source": [
    "some_gradient_classifier = GradientBoostingClassifier()\n",
    "some_gradient_classifier.fit(X_train, y_train)\n",
    "predictions = some_gradient_classifier.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на работу алгоритма при разном количестве эстиматоров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T18:22:50.074388Z",
     "start_time": "2019-12-23T18:22:50.070398Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score as score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T20:00:21.493724Z",
     "start_time": "2019-12-19T20:00:21.483713Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8447565434391504"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(y_val, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T20:01:53.229182Z",
     "start_time": "2019-12-19T20:01:52.947937Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8369138369138369"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = some_gradient_classifier.predict(X_test)\n",
    "score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T20:03:19.984724Z",
     "start_time": "2019-12-19T20:02:40.812845Z"
    }
   },
   "outputs": [],
   "source": [
    "some_gradient_classifier = GradientBoostingClassifier(n_estimators=100)\n",
    "some_gradient_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T20:04:32.064163Z",
     "start_time": "2019-12-19T20:04:31.484180Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8524171551273867"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = some_gradient_classifier.predict(X_val)\n",
    "score(predictions, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T20:04:54.833147Z",
     "start_time": "2019-12-19T20:04:54.044232Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8434588434588435"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = some_gradient_classifier.predict(X_test)\n",
    "score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T20:06:34.569390Z",
     "start_time": "2019-12-19T20:05:55.325292Z"
    }
   },
   "outputs": [],
   "source": [
    "some_gradient_classifier = GradientBoostingClassifier(n_estimators=100, lr=.25)\n",
    "some_gradient_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T20:06:43.969955Z",
     "start_time": "2019-12-19T20:06:43.370945Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8504439672682955"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = some_gradient_classifier.predict(X_val)\n",
    "score(predictions, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T20:06:50.782703Z",
     "start_time": "2019-12-19T20:06:49.977855Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8411488411488411"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = some_gradient_classifier.predict(X_test)\n",
    "score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T20:08:40.717619Z",
     "start_time": "2019-12-19T20:07:24.857471Z"
    }
   },
   "outputs": [],
   "source": [
    "some_gradient_classifier = GradientBoostingClassifier(n_estimators=200)\n",
    "some_gradient_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T20:08:59.028943Z",
     "start_time": "2019-12-19T20:08:57.840096Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8559572862863444"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = some_gradient_classifier.predict(X_val)\n",
    "score(predictions, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T20:09:15.804789Z",
     "start_time": "2019-12-19T20:09:14.218032Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8457303457303458"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = some_gradient_classifier.predict(X_test)\n",
    "score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### подберем оптимальный learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T20:10:57.152117Z",
     "start_time": "2019-12-19T20:10:57.147131Z"
    }
   },
   "outputs": [],
   "source": [
    "lrs = [0.1 * i for i in range(1, 11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T20:32:44.066837Z",
     "start_time": "2019-12-19T20:19:02.129166Z"
    }
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "for lr in lrs:\n",
    "    some_gradient_classifier = GradientBoostingClassifier(n_estimators=200, lr=lr)\n",
    "    some_gradient_classifier.fit(X_train, y_train)\n",
    "    predictions = some_gradient_classifier.predict(X_val)\n",
    "    scores.append(score(predictions, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T20:34:14.573626Z",
     "start_time": "2019-12-19T20:34:14.567642Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T20:34:21.364342Z",
     "start_time": "2019-12-19T20:34:21.358359Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8502118275201671,\n",
       " 0.8533457141199002,\n",
       " 0.8522430503162904,\n",
       " 0.855144797167895,\n",
       " 0.8539840984272532,\n",
       " 0.8542742731124137,\n",
       " 0.8539260634902212,\n",
       " 0.8548546224827346,\n",
       " 0.8539840984272532,\n",
       " 0.8540421333642852]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T20:34:33.899074Z",
     "start_time": "2019-12-19T20:34:33.893091Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrs[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### оптимальные параметры для градиентного бустинга ниже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T20:37:40.751267Z",
     "start_time": "2019-12-19T20:35:08.087423Z"
    }
   },
   "outputs": [],
   "source": [
    "some_gradient_classifier = GradientBoostingClassifier(n_estimators=400, lr=0.4)\n",
    "some_gradient_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T20:38:13.021876Z",
     "start_time": "2019-12-19T20:38:10.420830Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = some_gradient_classifier.predict(X_val)\n",
    "score(predictions, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T20:38:23.226917Z",
     "start_time": "2019-12-19T20:38:19.924745Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8475398475398476"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = some_gradient_classifier.predict(X_test)\n",
    "score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### подберем теперь случайный лес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T20:48:30.081407Z",
     "start_time": "2019-12-19T20:48:00.131411Z"
    }
   },
   "outputs": [],
   "source": [
    "some_random_forest = RandomForestClassifier()\n",
    "some_random_forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T20:49:51.769639Z",
     "start_time": "2019-12-19T20:49:51.462460Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8473681156055946"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = some_random_forest.predict(X_val)\n",
    "score(predictions, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T20:50:12.908770Z",
     "start_time": "2019-12-19T20:50:12.476900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8375298375298376"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = some_random_forest.predict(X_test)\n",
    "score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T20:54:29.898994Z",
     "start_time": "2019-12-19T20:50:44.317765Z"
    }
   },
   "outputs": [],
   "source": [
    "some_random_forest = RandomForestClassifier(n_estimators=200)\n",
    "some_random_forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T20:54:33.041597Z",
     "start_time": "2019-12-19T20:54:29.905980Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8521850153792583"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = some_random_forest.predict(X_val)\n",
    "score(predictions, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T20:54:36.575154Z",
     "start_time": "2019-12-19T20:54:33.044593Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8421113421113421"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = some_random_forest.predict(X_test)\n",
    "score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### оптимальные параметры алгоритмов и результат их работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:48:48.493706Z",
     "start_time": "2019-12-23T19:46:05.449050Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Classifier:\n",
      "Validation accuracy = 0.8559572862863444\n",
      "Test accuracy = 0.8463078463078463\n"
     ]
    }
   ],
   "source": [
    "some_gradient_classifier = GradientBoostingClassifier(n_estimators=400, lr=0.4)\n",
    "some_gradient_classifier.fit(X_train, y_train)\n",
    "predictions_val = some_gradient_classifier.predict(X_val)\n",
    "score_val = score(predictions_val, y_val)\n",
    "predictions_test = some_gradient_classifier.predict(X_test)\n",
    "score_test = score(predictions_test, y_test)\n",
    "print(f'Gradient Boosting Classifier:\\nValidation accuracy = {score_val}\\nTest accuracy = {score_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:48:48.502683Z",
     "start_time": "2019-12-23T19:48:48.496701Z"
    }
   },
   "outputs": [],
   "source": [
    "assert(score_val > 0.845 and score_test > 0.845)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:52:44.155963Z",
     "start_time": "2019-12-23T19:48:48.506673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier:\n",
      "Validation accuracy = 0.8534037490569323\n",
      "Test accuracy = 0.8423808423808424\n"
     ]
    }
   ],
   "source": [
    "some_random_forest = RandomForestClassifier(n_estimators=200)\n",
    "some_random_forest.fit(X_train, y_train)\n",
    "predictions_val = some_random_forest.predict(X_val)\n",
    "score_val = score(predictions_val, y_val)\n",
    "predictions_test = some_random_forest.predict(X_test)\n",
    "score_test = score(predictions_test, y_test)\n",
    "print(f'Random Forest Classifier:\\nValidation accuracy = {score_val}\\nTest accuracy = {score_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:52:44.176015Z",
     "start_time": "2019-12-23T19:52:44.163204Z"
    }
   },
   "outputs": [],
   "source": [
    "assert(score_val > 0.84 and score_test > 0.84)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 AdaBoost\n",
    "\n",
    "В алгоритме AdaBoost всем объектам обучения присваивается вес `weight`, который определяет степень важности объекта при обучении. И если текущая модель ошибается на некотором объекте, его вес увеличивается, и этот объект будет больше влиять на обучение следующей модели. Также, так как модели обучаются последовательно, они не равносильны между собой, поэтому у каждой модели тоже есть вес `alpha`, который определяет вес модели при суммировании ответов и зависит от количества ошибок `err` модели. На каждой итерации обучения, эти веса пересчитываются по формулам:\n",
    "\n",
    "* $$\\alpha_j = \\log\\left(\\frac{1-err_j}{err_j}\\right),$$\n",
    "где $err_j$ - ошибка классификации\n",
    "\n",
    "* $$w_{new}^t = \\frac{w_{old}^{t}*\\exp(-\\alpha_j \\cdot y(x^t) \\cdot b_j(x^t))}{\\sum\\limits_{i=1}^m w_{old}^{t}*\\exp(-\\alpha_j \\cdot y(x^i) \\cdot b_j(x^i))}$$\n",
    "Изначально все веса объектов $w^i$ равны (и нормированы на 1).\n",
    "\n",
    "Реализуем AdaBoost, учтя следующее:\n",
    "* надо работать с метками {-1,1} - это обусловлено использованием экспоненциальной функции потерь\n",
    "* метод `predict` представляет собой функцию сигмоид, примененную к сумме предсказаний всех моделей  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:27:28.130991Z",
     "start_time": "2019-12-23T19:27:28.114997Z"
    }
   },
   "outputs": [],
   "source": [
    "class AdaboostClassifier:\n",
    "    estimators = []\n",
    "    weights = []\n",
    "    def __init__(self, n_estimators=10, learning_rate=0.5):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # initial weights for samples\n",
    "        w = np.ones(X.shape[0]) / X.shape[0]\n",
    "        \n",
    "        # fit each estimator\n",
    "        for i in range(self.n_estimators):\n",
    "            # fit base estimator\n",
    "            clf = DecisionTreeClassifier(max_depth=1).fit(X, y, sample_weight=w)\n",
    "            \n",
    "            # get predict from estimator\n",
    "            preds = clf.predict(X)\n",
    "            \n",
    "            # indicator\n",
    "            miss = (preds != y).astype(int)\n",
    "            \n",
    "            # indicator -1, 1 to do exponent right\n",
    "            miss2 = np.where(miss == 0, -1, 1)\n",
    "            \n",
    "            # error\n",
    "            error = w @ miss / np.sum(w)\n",
    "            \n",
    "            # calculate weight of estimator\n",
    "            weight = self.learning_rate * np.log((1 - error) / error)\n",
    "            \n",
    "            # update weights\n",
    "            w = w * np.exp(miss2 * weight)\n",
    "            \n",
    "            # append estimator and weight to classifier\n",
    "            self.estimators.append(clf)\n",
    "            self.weights.append(weight)\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        preds = np.zeros(X.shape[0])\n",
    "        for estimator, weight in zip(self.estimators, self.weights):\n",
    "            preds += weight * estimator.predict(X)\n",
    "        return np.sign(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Основные идеи имплементации взяты из [этой](https://towardsdatascience.com/machine-learning-part-17-boosting-algorithms-adaboost-in-python-d00faac6c464) статьи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомним, что для адабуста нужны метки -1 и 1, а не 0 и 1, поэтому в таргете заменим 0 на -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:52:44.207925Z",
     "start_time": "2019-12-23T19:52:44.181997Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test[y_test == 0] = -1\n",
    "y_val[y_val == 0] = -1\n",
    "y_train[y_train == 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:27:29.727534Z",
     "start_time": "2019-12-23T19:27:29.306662Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.815684729462157\n",
      "0.8220648830596019\n",
      "0.8084623084623085\n"
     ]
    }
   ],
   "source": [
    "adaboost = AdaboostClassifier(n_estimators=2)\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "preds = adaboost.predict(X_train)\n",
    "\n",
    "print(score(preds, y_train)) #0.815\n",
    "\n",
    "preds = adaboost.predict(X_val)\n",
    "print(score(preds, y_val))\n",
    "\n",
    "preds = adaboost.predict(X_test)\n",
    "print(score(preds, y_test))\n",
    "#815, 822, 808"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:19:41.201577Z",
     "start_time": "2019-12-23T19:19:22.738027Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8351517355639068"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adaboost = AdaboostClassifier(n_estimators=100)\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "preds = adaboost.predict(X_train)\n",
    "\n",
    "score(preds, y_train) #0.815\n",
    "\n",
    "preds = adaboost.predict(X_val)\n",
    "score(preds, y_val)\n",
    "\n",
    "preds = adaboost.predict(X_test)\n",
    "score(preds, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:21:58.434568Z",
     "start_time": "2019-12-23T19:21:12.157469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8389905847173394\n",
      "0.8398816087284545\n",
      "0.8298298298298298\n"
     ]
    }
   ],
   "source": [
    "adaboost = AdaboostClassifier(n_estimators=200)\n",
    "adaboost.fit(X_train, y_train)\n",
    "\n",
    "preds = adaboost.predict(X_train)\n",
    "\n",
    "print(score(preds, y_train)) #0.815\n",
    "\n",
    "preds = adaboost.predict(X_val)\n",
    "print(score(preds, y_val))\n",
    "\n",
    "preds = adaboost.predict(X_test)\n",
    "print(score(preds, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:38:09.045267Z",
     "start_time": "2019-12-23T19:36:47.392036Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaboostClassifier\n",
      "Train accuracy = 0.8398189679557118\n",
      "Validation accuracy = 0.8401137484765829\n",
      "Test accuracy = 0.8321398321398321\n"
     ]
    }
   ],
   "source": [
    "adaboost = AdaboostClassifier(n_estimators=300)\n",
    "adaboost.fit(X_train, y_train)\n",
    "print('AdaboostClassifier')\n",
    "preds = adaboost.predict(X_train)\n",
    "print(f'Train accuracy = {score(preds, y_train)}')\n",
    "preds = adaboost.predict(X_val)\n",
    "print(f'Validation accuracy = {score(preds, y_val)}')\n",
    "preds = adaboost.predict(X_test)\n",
    "print(f'Test accuracy = {score(preds, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~о май гад, на валидации скор лучше, чем на трейне, кто-нибудь остановите адабуст~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оптимальные параметры для adaboost и результат работы алгоритма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:54:56.201152Z",
     "start_time": "2019-12-23T19:53:46.654447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaboost Classifier:\n",
      "Validation accuracy = 0.8423771110208346\n",
      "Test accuracy = 0.8352198352198352\n"
     ]
    }
   ],
   "source": [
    "adaboost = AdaboostClassifier(n_estimators=300)\n",
    "adaboost.fit(X_train, y_train)\n",
    "preds_val = adaboost.predict(X_val)\n",
    "preds_test = adaboost.predict(X_test)\n",
    "score_val = score(preds_val, y_val)\n",
    "score_test = score(preds_test, y_test)\n",
    "print(f'Adaboost Classifier:\\nValidation accuracy = {score_val}\\nTest accuracy = {score_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-23T19:54:56.212123Z",
     "start_time": "2019-12-23T19:54:56.206139Z"
    }
   },
   "outputs": [],
   "source": [
    "assert(score_val > 0.83 and score_test > 0.83)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Некоторые наблюдения по адабусту в ходе экспериментов. Дико переобучается, если в выборке есть шумы/выбросы, и это не фиксится даже лернинг рейтом. Но решение есть: обучить, посмотреть распределение весов, если веса большие у объектов, то это скорее всего выбросы, удалить их из выборки, обучить заново на новой выборки (решение, конечно, так себе, т.к. обучаться долго будет...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
